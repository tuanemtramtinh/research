{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d7824ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, TypedDict\n",
    "import spacy\n",
    "from textacy import extract, preprocessing\n",
    "from typing import Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from spacy.tokens import Doc\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "152f637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "#Danh sách từ không được tính là Actors\n",
    "INTERNAL_SYSTEM_KEYWORDS = {\"system\", \"software\", \"application\", \"app\", \"platform\"}\n",
    "\n",
    "model = init_chat_model(\n",
    "  \"gpt-5-mini\",\n",
    "  model_provider=\"openai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "59327fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorAlias(BaseModel):\n",
    "  actor: str = Field(description=\"The original actor's name\")\n",
    "  aliases: List[str] = Field(description=\"List of alternative names/references for this actor\")\n",
    "  sentences: List[str] = Field(description=\"Sentences where the actor or its aliases appear\")\n",
    "  \n",
    "  def __str__(self) -> str:\n",
    "    # Gom các alias lại bằng dấu phẩy\n",
    "    alias_str = \", \".join(self.aliases) if self.aliases else \"None\"\n",
    "    \n",
    "    # Format các câu sentence thành dạng danh sách có gạch đầu dòng\n",
    "    sentences_str = \"\\n    - \".join(self.sentences)\n",
    "    \n",
    "    return (\n",
    "        f\"ACTOR: {self.actor}\\n\"\n",
    "        f\"Aliases: {alias_str}\\n\"\n",
    "        f\"Sentences:\\n    - {sentences_str}\\n\"\n",
    "        f\"{'-'*30}\"\n",
    "    )\n",
    "  \n",
    "class ActorList(BaseModel):\n",
    "  actors: List[str] = Field(description=\"A list of actors who perform actions in the requirement.\")\n",
    "  \n",
    "class ActorAliasMapping(BaseModel):\n",
    "  mappings: List[ActorAlias] = Field(description=\"List of actor-alias mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0f77358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo State cho Graph\n",
    "class GraphState(TypedDict):\n",
    "  input_text: str     #Yêu cầu đầu vào của người dùng\n",
    "  doc: Doc            #Doc sau khi gọi hàm nlp của SpaCy\n",
    "  actors: List[str]   #Danh sách actors cuối cùng\n",
    "  actor_aliases: List[ActorAlias] #Danh sách các alias của actor\n",
    "  svo_elements: Annotated[List[Dict[str,str]], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f180e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actors node\n",
    "def actors_node(state: GraphState):\n",
    "  structured_llm = model.with_structured_output(ActorList)\n",
    "  candidate_chunks = \", \".join(state[\"actors\"])\n",
    "  \n",
    "  system_prompt = (\n",
    "    \"You are a Senior Systems Analyst and Linguistic Expert. Your task is to perform \"\n",
    "    \"Entity Extraction specifically for 'Actors' in a system description or user story. \"\n",
    "    \"An 'Actor' is defined as a person, organization, or external system that \"\n",
    "    \"performs actions, initiates processes, or interacts with the system described.\"\n",
    ")\n",
    "\n",
    "  user_prompt = f\"\"\"\n",
    "  I will provide you with a raw text and a list of potential 'Noun Chunks' extracted by a parser.\n",
    "\n",
    "  ### RULES:\n",
    "  1. **Filtering**: From the 'Candidate Noun Chunks', select only those that function as an active agent (Actor) in the 'Raw Text'.\n",
    "  2. **Standardization**: Convert all extracted actors to their **singular form** (e.g., 'customers' -> 'customer').\n",
    "  3. **Cleaning**: Remove any unnecessary articles (a, an, the) and honorifics.\n",
    "  4. **Context Check**: Ensure the noun chunk is actually performing an action in the text, not just being mentioned as an object.\n",
    "  5. **Exclude Self-References**: Do NOT include 'the system', 'the software', or 'the application' as an Actor if it refers to the system being described. These are internal components, not external actors.\n",
    "  6. **External Systems**: Only include other specific systems if they are external entities that your system interacts with (e.g., 'Payment Gateway', 'External Database').\n",
    "  \n",
    "  ### INPUT DATA:\n",
    "  - Raw Text: {state['input_text']}\n",
    "  - Candidate Noun Chunks: {candidate_chunks}\n",
    "\n",
    "  ### OUTPUT INSTRUCTIONS:\n",
    "  Return only the final list of singularized actors.\n",
    "  \"\"\"\n",
    "    \n",
    "  response: ActorList = structured_llm.invoke([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt)\n",
    "  ])\n",
    "  \n",
    "  # print(response)\n",
    "  \n",
    "  return {\n",
    "    \"actors\": response.actors\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8c659498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actors_alias_node\n",
    "\n",
    "def actors_alias_node(state: GraphState):\n",
    "  sentences = state.get(\"doc\").sents\n",
    "  \n",
    "  structured_llm = model.with_structured_output(ActorAliasMapping)\n",
    "  \n",
    "  # Đầu vào cho LLMs\n",
    "  input_sentences = [sentence.text.strip() for sentence in sentences]\n",
    "  input_sentences = \"\\n\".join([f\"{i + 1}. {s}\" for i, s in enumerate(input_sentences)])\n",
    "  \n",
    "  actors_list = \", \".join(state.get(\"actors\"))\n",
    "  \n",
    "  system_prompt = (\n",
    "    \"You are an expert in Natural Language Processing and Entity Resolution. \"\n",
    "    \"Your task is to identify all alternative references (aliases) to specific actors \"\n",
    "    \"in a given text and determine which sentences contain these references.\"\n",
    "  )\n",
    "  \n",
    "  user_prompt = f\"\"\"\n",
    "  Analyze the following pre-segmented numbered sentences and identify all references to the given actors.\n",
    "\n",
    "  ### ACTORS TO TRACK:\n",
    "  {actors_list}\n",
    "\n",
    "  ### PRE-SEGMENTED SENTENCES:\n",
    "  {input_sentences}\n",
    "\n",
    "  ### ADVANTAGES OF PRE-SEGMENTATION:\n",
    "  - Sentences are already properly split by a trained NLP model\n",
    "  - Sentence boundaries are accurate (handles abbreviations, numbers, etc.)\n",
    "  - You can focus on coreference resolution instead of sentence splitting\n",
    "\n",
    "  ### TASK:\n",
    "  For each actor:\n",
    "  1. Find all ways it is referenced (direct mentions, pronouns, role titles, etc.)\n",
    "  2. List all unique aliases/references  \n",
    "  3. Reference sentences by their NUMBER (e.g., \"1\", \"3\", \"5\")\n",
    "\n",
    "  ### COREFERENCE RULES:\n",
    "  - Track pronoun chains across sentences (e.g., sent 1: \"customer\" -> sent 2: \"he\" -> sent 3: \"the user\")\n",
    "  - Resolve possessive pronouns (his, her, their) to their antecedents\n",
    "  - Identify role-based references (the admin, the buyer, etc.)\n",
    "  - Connect demonstrative references (this person, that user)\n",
    "  - Consider sentence proximity for ambiguous references\n",
    "\n",
    "  ### OUTPUT FORMAT:\n",
    "  In the 'sentences' field, include BOTH the number AND the full text:\n",
    "  Example: \"1. The customer logs in.\"\n",
    "\n",
    "  This allows for easy verification and mapping back to the source.\n",
    "  \"\"\"\n",
    "  \n",
    "  response: ActorAliasMapping = structured_llm.invoke([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", user_prompt)\n",
    "  ])\n",
    "  \n",
    "  return {\n",
    "    \"actor_aliases\": response.mappings\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7c784d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"actors_node\", actors_node)\n",
    "workflow.add_node(\"actors_alias_node\", actors_alias_node)\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"actors_node\")\n",
    "workflow.add_edge(\"actors_node\", \"actors_alias_node\")\n",
    "workflow.add_edge(\"actors_alias_node\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "82938c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['librarian', 'student', 'automated kiosk', 'administrator']\n",
      "ACTOR: librarian\n",
      "Aliases: The librarians, librarians\n",
      "Sentences:\n",
      "    - 1. The librarians manage the digital catalog by adding new book records and updating existing entries.\n",
      "------------------------------\n",
      "ACTOR: student\n",
      "Aliases: a student, the student, their, borrowers, the borrowers, student\n",
      "Sentences:\n",
      "    - 2. When a student searches for a book, the system displays the availability status in real-time.\n",
      "    - 3. If the student decides to borrow a book, the automated kiosk scans their ID card and records the transaction in the database.\n",
      "    - 4. After the loan period expires, the management system sends email notifications to the borrowers.\n",
      "------------------------------\n",
      "ACTOR: automated kiosk\n",
      "Aliases: the automated kiosk, automated kiosk\n",
      "Sentences:\n",
      "    - 3. If the student decides to borrow a book, the automated kiosk scans their ID card and records the transaction in the database.\n",
      "------------------------------\n",
      "ACTOR: administrator\n",
      "Aliases: administrators, the administrators\n",
      "Sentences:\n",
      "    - 5. Meanwhile, administrators generate monthly reports to monitor library usage and track late returns.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./input.txt\", \"r\", encoding=\"UTF-8\") as f:\n",
    "  input = f.read()\n",
    "  \n",
    "input = preprocessing.normalize.whitespace(input)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(input)\n",
    "\n",
    "chunks = extract.noun_chunks(doc, min_freq=1)\n",
    "chunks = [chunk.text for chunk in chunks if chunk.root.pos_ != 'PRON' and chunk.root.lemma_.lower() not in INTERNAL_SYSTEM_KEYWORDS]\n",
    "\n",
    "\n",
    "#Trạng thái khởi tạo ban đầu cho process\n",
    "initial_state = {\n",
    "  \"input_text\": input,\n",
    "  \"doc\": doc,\n",
    "  \"actors\": chunks,\n",
    "  \"actor_aliases\": [],\n",
    "  \"svo_elements\": [],\n",
    "}\n",
    "\n",
    "result: GraphState = app.invoke(initial_state)\n",
    "\n",
    "print(result.get(\"actors\"))\n",
    "for item in result.get(\"actor_aliases\"):\n",
    "  print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
